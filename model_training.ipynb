{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "import topcoder_mongo as DB\n",
    "import topcoder_ml as TML\n",
    "import static_var as S\n",
    "import util as U\n",
    "\n",
    "sns.set(\n",
    "    rc={\n",
    "        'axes.facecolor':'#121212',\n",
    "        'figure.facecolor':'#121212',\n",
    "        'text.color': 'white',\n",
    "        'axes.titlecolor': 'white',\n",
    "        'axes.labelcolor': 'white',\n",
    "        'xtick.color': 'white',\n",
    "        'ytick.color': 'white',\n",
    "        'figure.autolayout': True,\n",
    "    },\n",
    ")\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-tongue",
   "metadata": {},
   "source": [
    "A practical problem that I can never figure out:\n",
    "\n",
    "**When should I standardize my dataset and should I standardize all features??**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-kazakhstan",
   "metadata": {},
   "source": [
    "## Retrieve training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature, target = TML.get_training_data()\n",
    "X, y = feature.to_numpy(), target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-citation",
   "metadata": {},
   "source": [
    "Let's visualize the distribution of `top2_prize`. I plot the frequency of different prize in a $50 interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-singapore",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target.top2_prize.min(), target.top2_prize.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-omaha",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bins = int((2700 - 300) / 50)\n",
    "fig, ax = plt.subplots(figsize=(16, 6.67), dpi=200)\n",
    "\n",
    "sns.histplot(x=target.top2_prize, bins=bins, lw=0.5, ax=ax)\n",
    "sns.despine(ax=ax, left=True)\n",
    "ax.set_xlim(300, 2700)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True, color='white', alpha=0.5)\n",
    "ax.set_title('Top2 Prize Distribution')\n",
    "ax.set_xlabel('Top2 Prize')\n",
    "ax.xaxis.set_major_locator(mticker.MultipleLocator(100))\n",
    "\n",
    "for p in ax.patches:\n",
    "    cnt = p.get_height()\n",
    "    x = p.get_x() + p.get_width() / 2\n",
    "    y = p.get_height()\n",
    "    ax.annotate(int(cnt), xy=(x, y), xytext=(x, y + 5), color='white', alpha=0.85, ha='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-subsection",
   "metadata": {},
   "source": [
    "I decide to run a Mongo query to get the challenge ids for each bin, because using `pandas` to achieve that will take more tweak and twist that just run a (still relatively complicated) query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-clarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "prize_intv_points = np.linspace(300, 2700, int((2700 - 300) / 50) + 1)\n",
    "prize_interval = list(zip(prize_interval, prize_interval[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-signal",
   "metadata": {},
   "source": [
    "## 10-Fold Cross Validation Predict\n",
    "\n",
    "### Cross Validation Strategy\n",
    "\n",
    "The \"Independent and Identically Distributed\" assumption that \n",
    "\n",
    "> _all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples_\n",
    "\n",
    "may not hold in the scenario of Topcoder dataset. So the following cross validation strategy will be used to split the training and testing sets.\n",
    "\n",
    "1. Split the dataset by `top2_prize` as if it's a classification problem. i.e. make sure different prizes are presented in the validation set.\n",
    "2. Split by `project_id` (assuming that challenges are dependant within each project)\n",
    "2. Split by `sub_track` (assuming that challenges are dependant within each sub-track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-slave",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-assault",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TopcoderPricingResearch2021",
   "language": "python",
   "name": "topcoderpricingresearch2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
